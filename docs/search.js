window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "agents", "modulename": "agents", "qualname": "", "type": "module", "doc": "<p>Agents module</p>\n"}, {"fullname": "agents.BTMAgent", "modulename": "agents.BTMAgent", "qualname": "", "type": "module", "doc": "<p>Beat The Mean (BTM) dueling bandit agent.\nInitially presented at https://www.cs.cornell.edu/people/tj/publications/yue_joachims_11a.pdf.</p>\n"}, {"fullname": "agents.BTMAgent.BTMAgent", "modulename": "agents.BTMAgent", "qualname": "BTMAgent", "type": "class", "doc": "<p>Implements a dueling bandit agent following the Beat the Mean policy.</p>\n"}, {"fullname": "agents.BTMAgent.BTMAgent.__init__", "modulename": "agents.BTMAgent", "qualname": "BTMAgent.__init__", "type": "function", "doc": "<p>Initializes BTM Agent. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms</li>\n<li><strong>horizon:</strong>  indicates the time horizon for the algorithm to run. </li>\n<li><strong>gamma:</strong>  represents transitivity relaxation.</li>\n</ul>\n", "parameters": ["self", "n_arms", "horizon", "gamma"], "funcdef": "def"}, {"fullname": "agents.BTMAgent.BTMAgent.reward", "modulename": "agents.BTMAgent", "qualname": "BTMAgent.reward", "type": "function", "doc": "<p>Updates the knowledge given the reward. Since it's a Dueling Bandit, the reward\nis a boolean indicating whether the first arm wins or not.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm_1:</strong>  first arm of the pulled pair.</li>\n<li><strong>n_arm_2:</strong>  second arm of the pulled pair.</li>\n<li><strong>one_wins:</strong>  boolean indicating whether the first arm won.</li>\n</ul>\n", "parameters": ["self", "n_arm_1", "n_arm_2", "one_wins"], "funcdef": "def"}, {"fullname": "agents.BTMAgent.BTMAgent.step", "modulename": "agents.BTMAgent", "qualname": "BTMAgent.step", "type": "function", "doc": "<p>(Override) Returns the pair that should be matched, using BTM</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Pair of indices (i,j) that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.BTMAgent.BTMAgent.reset", "modulename": "agents.BTMAgent", "qualname": "BTMAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.BTMAgent.BTMAgent.get_name", "modulename": "agents.BTMAgent", "qualname": "BTMAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.CCBAgent", "modulename": "agents.CCBAgent", "qualname": "", "type": "module", "doc": "<p>Copeland Confidence Bound (CCB) Dueling Bandit Agent.\nInitially presented at https://arxiv.org/abs/1506.00312.</p>\n"}, {"fullname": "agents.CCBAgent.CCBAgent", "modulename": "agents.CCBAgent", "qualname": "CCBAgent", "type": "class", "doc": "<p>Implements a dueling bandit agent following the CCB policy.</p>\n"}, {"fullname": "agents.CCBAgent.CCBAgent.__init__", "modulename": "agents.CCBAgent", "qualname": "CCBAgent.__init__", "type": "function", "doc": "<p>Initializes CCB agent.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms</li>\n<li><strong>alpha:</strong>  \"exploration rate\" similar to UCB.</li>\n</ul>\n", "parameters": ["self", "n_arms", "alpha"], "funcdef": "def"}, {"fullname": "agents.CCBAgent.CCBAgent.step", "modulename": "agents.CCBAgent", "qualname": "CCBAgent.step", "type": "function", "doc": "<p>(Override) Returns the pair that should be matched, using CCB.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Pair of indices (i,j) that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.CCBAgent.CCBAgent.reset", "modulename": "agents.CCBAgent", "qualname": "CCBAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.CCBAgent.CCBAgent.get_name", "modulename": "agents.CCBAgent", "qualname": "CCBAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.DBAgent", "modulename": "agents.DBAgent", "qualname": "", "type": "module", "doc": "<p>Generic Dueling Bandit Agent class.\nOverriding this class allows for custom agent implementations.</p>\n"}, {"fullname": "agents.DBAgent.DBAgent", "modulename": "agents.DBAgent", "qualname": "DBAgent", "type": "class", "doc": "<p>Abstract class for DB Agent</p>\n"}, {"fullname": "agents.DBAgent.DBAgent.__init__", "modulename": "agents.DBAgent", "qualname": "DBAgent.__init__", "type": "function", "doc": "<p>Initializes MABAgent object.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  Number of arms</li>\n<li><strong>optimism:</strong>  starting value for rewards</li>\n</ul>\n", "parameters": ["self", "n_arms"], "funcdef": "def"}, {"fullname": "agents.DBAgent.DBAgent.reward", "modulename": "agents.DBAgent", "qualname": "DBAgent.reward", "type": "function", "doc": "<p>Updates the knowledge given the reward. Since it's a Dueling Bandit, the reward\nis a boolean indicating whether the first arm wins or not.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm_1:</strong>  first arm of the pulled pair.</li>\n<li><strong>n_arm_2:</strong>  second arm of the pulled pair.</li>\n<li><strong>one_wins:</strong>  boolean indicating whether the first arm won.</li>\n</ul>\n", "parameters": ["self", "n_arm_1", "n_arm_2", "one_wins"], "funcdef": "def"}, {"fullname": "agents.DBAgent.DBAgent.step", "modulename": "agents.DBAgent", "qualname": "DBAgent.step", "type": "function", "doc": "<p>Returns the pair that should be matched. Override to use custom policy.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Pair of indices (i,j) that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.DBAgent.DBAgent.reset", "modulename": "agents.DBAgent", "qualname": "DBAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.DBAgent.DBAgent.get_ratio", "modulename": "agents.DBAgent", "qualname": "DBAgent.get_ratio", "type": "function", "doc": "<p>Returns the ratio of wins for n_arm_1 against n_arm_2.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm_1:</strong>  First arm to compare.</li>\n<li><strong>n_arm_2:</strong>  Second arm to compare.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>ratio of wins for n_arm_1 against n_arm_2, that is, empirical\n  estimation of the probability that n_arm_1 beats n_arm_2. Defaults\n  to 1/2 if no matches have been recorded.</p>\n</blockquote>\n", "parameters": ["self", "n_arm_1", "n_arm_2"], "funcdef": "def"}, {"fullname": "agents.DBAgent.DBAgent.get_comparison_count", "modulename": "agents.DBAgent", "qualname": "DBAgent.get_comparison_count", "type": "function", "doc": "<p>Returns the number of comparisons between the two arms specified.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm_1:</strong>  First arm to compare.</li>\n<li><strong>n_arm_2:</strong>  Second arm to compare.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>number of matches recorded between n_arm_1 and n_arm_2 (independent of order).</p>\n</blockquote>\n", "parameters": ["self", "n_arm_1", "n_arm_2"], "funcdef": "def"}, {"fullname": "agents.DBAgent.DBAgent.get_total_comparison_count", "modulename": "agents.DBAgent", "qualname": "DBAgent.get_total_comparison_count", "type": "function", "doc": "<p>Gets the total comparison count.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>total matches recorded.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.DBAgent.DBAgent.get_name", "modulename": "agents.DBAgent", "qualname": "DBAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.DTSAgent", "modulename": "agents.DTSAgent", "qualname": "", "type": "module", "doc": "<p>Double Thompson Sampling (DTS) Dueling Bandit Agent.\nFirst introduced in https://www.researchgate.net/publication/301648067_Double_Thompson_Sampling_for_Dueling_Bandits.</p>\n"}, {"fullname": "agents.DTSAgent.DTSAgent", "modulename": "agents.DTSAgent", "qualname": "DTSAgent", "type": "class", "doc": "<p>Abstract class for DB Agent</p>\n"}, {"fullname": "agents.DTSAgent.DTSAgent.__init__", "modulename": "agents.DTSAgent", "qualname": "DTSAgent.__init__", "type": "function", "doc": "<p>Initializes Double Thompson Sampling agent. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms.</li>\n<li><strong>alpha:</strong>  Starting alpha parameter for thompson sampling</li>\n<li><strong>beta:</strong>  Starting beta parameter for thompson sampling</li>\n<li><strong>gamma:</strong>  Size of the confidence interval for the starting UCB-like pruning phase.</li>\n</ul>\n", "parameters": ["self", "n_arms", "alpha", "beta", "gamma"], "funcdef": "def"}, {"fullname": "agents.DTSAgent.DTSAgent.step", "modulename": "agents.DTSAgent", "qualname": "DTSAgent.step", "type": "function", "doc": "<p>(Override) Returns the pair that should be matched, using DTS.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Pair of indices (i,j) that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.DTSAgent.DTSAgent.reset", "modulename": "agents.DTSAgent", "qualname": "DTSAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.DTSAgent.DTSAgent.get_name", "modulename": "agents.DTSAgent", "qualname": "DTSAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.DoublerAgent", "modulename": "agents.DoublerAgent", "qualname": "", "type": "module", "doc": "<p>Doubler (MAB reduction) dueling bandit agent.\nFirst introduced in http://proceedings.mlr.press/v32/ailon14.pdf.</p>\n"}, {"fullname": "agents.DoublerAgent.DoublerAgent", "modulename": "agents.DoublerAgent", "qualname": "DoublerAgent", "type": "class", "doc": "<p>Implements a dueling bandit agent following the Doubler policy.</p>\n"}, {"fullname": "agents.DoublerAgent.DoublerAgent.__init__", "modulename": "agents.DoublerAgent", "qualname": "DoublerAgent.__init__", "type": "function", "doc": "<p>Initializes Doubler agent. This agent allows a MAB to be used\nwith dueling bandits.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms</li>\n<li><strong>mab:</strong>  Object of type MABAgent that will be used for doubler.</li>\n</ul>\n", "parameters": ["self", "n_arms", "mab"], "funcdef": "def"}, {"fullname": "agents.DoublerAgent.DoublerAgent.reward", "modulename": "agents.DoublerAgent", "qualname": "DoublerAgent.reward", "type": "function", "doc": "<p>Updates the knowledge given the reward. Since it's a Dueling Bandit, the reward\nis a boolean indicating whether the first arm wins or not.\nIn Doubler, it feeds the result to the MAB. We assume in this implementation that\nthe arm played by the MAB is arm 1.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm_1:</strong>  first arm of the pulled pair.</li>\n<li><strong>n_arm_2:</strong>  second arm of the pulled pair.</li>\n<li><strong>one_wins:</strong>  boolean indicating whether the first arm won.</li>\n</ul>\n", "parameters": ["self", "n_arm_1", "n_arm_2", "one_wins"], "funcdef": "def"}, {"fullname": "agents.DoublerAgent.DoublerAgent.step", "modulename": "agents.DoublerAgent", "qualname": "DoublerAgent.step", "type": "function", "doc": "<p>(Override) Returns the pair that should be matched, using Doubler</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Pair of indices (i,j) that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.DoublerAgent.DoublerAgent.reset", "modulename": "agents.DoublerAgent", "qualname": "DoublerAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.DoublerAgent.DoublerAgent.get_name", "modulename": "agents.DoublerAgent", "qualname": "DoublerAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.EXP3Agent", "modulename": "agents.EXP3Agent", "qualname": "", "type": "module", "doc": "<p>EXP3 MAB agent.\nIntroduced in https://cseweb.ucsd.edu/~yfreund/papers/bandits.pdf.</p>\n"}, {"fullname": "agents.EXP3Agent.EXP3Gamma", "modulename": "agents.EXP3Agent", "qualname": "EXP3Gamma", "type": "function", "doc": "<p>Helper function to obtain \"theoretically good\" value for the exploration rate.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>best_value:</strong>  upper bound for the reward values</li>\n<li><strong>n_rounds:</strong>  number of iterations for EXP3</li>\n<li><strong>n_arms:</strong>  number of arms</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>value that should be given to EXP3's exploration rate.</p>\n</blockquote>\n", "parameters": ["best_value", "n_rounds", "n_arms"], "funcdef": "def"}, {"fullname": "agents.EXP3Agent.EXP3Agent", "modulename": "agents.EXP3Agent", "qualname": "EXP3Agent", "type": "class", "doc": "<p>Implements a multi armed bandit agent following the EXP3 policy.</p>\n"}, {"fullname": "agents.EXP3Agent.EXP3Agent.__init__", "modulename": "agents.EXP3Agent", "qualname": "EXP3Agent.__init__", "type": "function", "doc": "<p>Initializes EXP3 Agent. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms</li>\n<li><strong>exploration_rate:</strong>  weight (0 to 1) that is given to exploration vs exploitation.</li>\n<li><strong>optimism:</strong>  starting estimation for the value of every arm.</li>\n</ul>\n", "parameters": ["self", "n_arms", "exploration_rate", "optimism"], "funcdef": "def"}, {"fullname": "agents.EXP3Agent.EXP3Agent.step", "modulename": "agents.EXP3Agent", "qualname": "EXP3Agent.step", "type": "function", "doc": "<p>(Override) Returns the arm that should be pulled, using EXP3.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Index i of the arm that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.EXP3Agent.EXP3Agent.reward", "modulename": "agents.EXP3Agent", "qualname": "EXP3Agent.reward", "type": "function", "doc": "<p>Updates the knowledge given the reward. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm:</strong>  pulled arm.</li>\n<li><strong>reward:</strong>  numerical reward obtained.</li>\n</ul>\n", "parameters": ["self", "n_arm", "reward"], "funcdef": "def"}, {"fullname": "agents.EXP3Agent.EXP3Agent.reset", "modulename": "agents.EXP3Agent", "qualname": "EXP3Agent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.EXP3Agent.EXP3Agent.get_name", "modulename": "agents.EXP3Agent", "qualname": "EXP3Agent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.EpsilonGreedyAgent", "modulename": "agents.EpsilonGreedyAgent", "qualname": "", "type": "module", "doc": "<p>Epsilon greedy MAB agent.</p>\n"}, {"fullname": "agents.EpsilonGreedyAgent.EpsilonGreedyAgent", "modulename": "agents.EpsilonGreedyAgent", "qualname": "EpsilonGreedyAgent", "type": "class", "doc": "<p>Implements a multi armed bandit agent following the epsilon greedy policy.</p>\n"}, {"fullname": "agents.EpsilonGreedyAgent.EpsilonGreedyAgent.__init__", "modulename": "agents.EpsilonGreedyAgent", "qualname": "EpsilonGreedyAgent.__init__", "type": "function", "doc": "<p>Initializes epsilon_greedy Agent. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms</li>\n<li><strong>epsilon:</strong>  probability of (random) exploration</li>\n<li><strong>optimism:</strong>  starting estimation for the value of every arm.</li>\n</ul>\n", "parameters": ["self", "n_arms", "epsilon", "optimism"], "funcdef": "def"}, {"fullname": "agents.EpsilonGreedyAgent.EpsilonGreedyAgent.step", "modulename": "agents.EpsilonGreedyAgent", "qualname": "EpsilonGreedyAgent.step", "type": "function", "doc": "<p>(Override) Returns the arm that should be pulled, using epsilon greedy.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Index i of the arm that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.EpsilonGreedyAgent.EpsilonGreedyAgent.get_name", "modulename": "agents.EpsilonGreedyAgent", "qualname": "EpsilonGreedyAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.IFAgent", "modulename": "agents.IFAgent", "qualname": "", "type": "module", "doc": "<p>Interleaved Filter (IF) dueling bandit agent.\nIntroduced in https://www.cs.cornell.edu/people/tj/publications/yue_etal_09a.pdf.</p>\n"}, {"fullname": "agents.IFAgent.IFAgent", "modulename": "agents.IFAgent", "qualname": "IFAgent", "type": "class", "doc": "<p>Implements a dueling bandit agent following the Interleaved Filter policy.</p>\n"}, {"fullname": "agents.IFAgent.IFAgent.__init__", "modulename": "agents.IFAgent", "qualname": "IFAgent.__init__", "type": "function", "doc": "<p>Initializes IF Agent. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms</li>\n<li><strong>horizon:</strong>  indicates the time horizon for the algorithm to run.</li>\n</ul>\n", "parameters": ["self", "n_arms", "horizon"], "funcdef": "def"}, {"fullname": "agents.IFAgent.IFAgent.reward", "modulename": "agents.IFAgent", "qualname": "IFAgent.reward", "type": "function", "doc": "<p>Updates the knowledge given the reward. Since it's a Dueling Bandit, the reward\nis a boolean indicating whether the first arm wins or not.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm_1:</strong>  first arm of the pulled pair.</li>\n<li><strong>n_arm_2:</strong>  second arm of the pulled pair.</li>\n<li><strong>one_wins:</strong>  boolean indicating whether the first arm won.</li>\n</ul>\n", "parameters": ["self", "n_arm_1", "n_arm_2", "one_wins"], "funcdef": "def"}, {"fullname": "agents.IFAgent.IFAgent.step", "modulename": "agents.IFAgent", "qualname": "IFAgent.step", "type": "function", "doc": "<p>(Override) Returns the pair that should be matched, using IF</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Pair of indices (i,j) that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.IFAgent.IFAgent.reset", "modulename": "agents.IFAgent", "qualname": "IFAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.IFAgent.IFAgent.get_name", "modulename": "agents.IFAgent", "qualname": "IFAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.MABAgent", "modulename": "agents.MABAgent", "qualname": "", "type": "module", "doc": "<p>Generic Multi Armed Bandit Agent class.\nOverriding this class allows for custom agent implementations.</p>\n"}, {"fullname": "agents.MABAgent.MABAgent", "modulename": "agents.MABAgent", "qualname": "MABAgent", "type": "class", "doc": "<p>Abstract class for MAB Agent</p>\n"}, {"fullname": "agents.MABAgent.MABAgent.__init__", "modulename": "agents.MABAgent", "qualname": "MABAgent.__init__", "type": "function", "doc": "<p>Initializes MABAgent object.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  Number of arms</li>\n<li><strong>optimism:</strong>  starting value for rewards</li>\n</ul>\n", "parameters": ["self", "n_arms", "optimism"], "funcdef": "def"}, {"fullname": "agents.MABAgent.MABAgent.reward", "modulename": "agents.MABAgent", "qualname": "MABAgent.reward", "type": "function", "doc": "<p>Updates the knowledge given the reward. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm:</strong>  pulled arm.</li>\n<li><strong>reward:</strong>  numerical reward obtained.</li>\n</ul>\n", "parameters": ["self", "n_arm", "reward"], "funcdef": "def"}, {"fullname": "agents.MABAgent.MABAgent.step", "modulename": "agents.MABAgent", "qualname": "MABAgent.step", "type": "function", "doc": "<p>(Override) Returns the arm that should be pulled, using EXP3.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Index i of the arm that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.MABAgent.MABAgent.reset", "modulename": "agents.MABAgent", "qualname": "MABAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.MABAgent.MABAgent.get_best", "modulename": "agents.MABAgent", "qualname": "MABAgent.get_best", "type": "function", "doc": "<p>Get the best arm prediction so far.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>index of the estimated best arm.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.MABAgent.MABAgent.get_name", "modulename": "agents.MABAgent", "qualname": "MABAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.MultiSBMAgent", "modulename": "agents.MultiSBMAgent", "qualname": "", "type": "module", "doc": "<p>MultiSBM (MAB reduction) dueling bandit agent.\nFirst introduced in http://proceedings.mlr.press/v32/ailon14.pdf.</p>\n"}, {"fullname": "agents.MultiSBMAgent.MultiSBMAgent", "modulename": "agents.MultiSBMAgent", "qualname": "MultiSBMAgent", "type": "class", "doc": "<p>Implements a dueling bandit agent following the MultiSBM policy.</p>\n"}, {"fullname": "agents.MultiSBMAgent.MultiSBMAgent.__init__", "modulename": "agents.MultiSBMAgent", "qualname": "MultiSBMAgent.__init__", "type": "function", "doc": "<p>Initializes MultiSBM agent. This agent allows a MAB to be used\nwith dueling bandits.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>mab_callable:</strong>  Callable that returns an object of type MABAgent\nto be used within the MultiSBM.</li>\n<li><strong>mab_args:</strong>  List of arguments to be passed to the MAB callable on\ncreation.</li>\n<li><strong>mab_kwargs:</strong>  Dict of key-word arguments to be passed to the MAB\ncallable on creation.</li>\n</ul>\n", "parameters": ["self", "n_arms", "mab_callable", "mab_args", "mab_kwargs"], "funcdef": "def"}, {"fullname": "agents.MultiSBMAgent.MultiSBMAgent.reward", "modulename": "agents.MultiSBMAgent", "qualname": "MultiSBMAgent.reward", "type": "function", "doc": "<p>Updates the knowledge given the reward. Since it's a Dueling Bandit, the reward\nis a boolean indicating whether the first arm wins or not.\nIn MultiSBM it feeds the comparison result to the agent in charge of facing arm 1.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm_1:</strong>  first arm of the pulled pair.</li>\n<li><strong>n_arm_2:</strong>  second arm of the pulled pair.</li>\n<li><strong>one_wins:</strong>  boolean indicating whether the first arm won.</li>\n</ul>\n", "parameters": ["self", "n_arm_1", "n_arm_2", "one_wins"], "funcdef": "def"}, {"fullname": "agents.MultiSBMAgent.MultiSBMAgent.step", "modulename": "agents.MultiSBMAgent", "qualname": "MultiSBMAgent.step", "type": "function", "doc": "<p>(Override) Returns the pair that should be matched, using MultiSBM</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Pair of indices (i,j) that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.MultiSBMAgent.MultiSBMAgent.reset", "modulename": "agents.MultiSBMAgent", "qualname": "MultiSBMAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.MultiSBMAgent.MultiSBMAgent.get_name", "modulename": "agents.MultiSBMAgent", "qualname": "MultiSBMAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.RUCBAgent", "modulename": "agents.RUCBAgent", "qualname": "", "type": "module", "doc": "<p>Relative UCB (RUCB) Dueling Bandit Agent.\nFirst introduced in: http://proceedings.mlr.press/v32/zoghi14.pdf</p>\n"}, {"fullname": "agents.RUCBAgent.RUCBAgent", "modulename": "agents.RUCBAgent", "qualname": "RUCBAgent", "type": "class", "doc": "<p>Implements a dueling bandit agent following the RUCB policy.</p>\n"}, {"fullname": "agents.RUCBAgent.RUCBAgent.__init__", "modulename": "agents.RUCBAgent", "qualname": "RUCBAgent.__init__", "type": "function", "doc": "<p>Initializes RUCB agent.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms. </li>\n<li><strong>alpha:</strong>  \"Exploration rate\" similar to UCB.</li>\n</ul>\n", "parameters": ["self", "n_arms", "alpha"], "funcdef": "def"}, {"fullname": "agents.RUCBAgent.RUCBAgent.step", "modulename": "agents.RUCBAgent", "qualname": "RUCBAgent.step", "type": "function", "doc": "<p>(Override) Returns the pair that should be matched, using RUCB.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Pair of indices (i,j) that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.RUCBAgent.RUCBAgent.reset", "modulename": "agents.RUCBAgent", "qualname": "RUCBAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.RUCBAgent.RUCBAgent.get_name", "modulename": "agents.RUCBAgent", "qualname": "RUCBAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.RandomAgent", "modulename": "agents.RandomAgent", "qualname": "", "type": "module", "doc": "<p>Random DB Agent to serve as baseline.</p>\n"}, {"fullname": "agents.RandomAgent.RandomAgent", "modulename": "agents.RandomAgent", "qualname": "RandomAgent", "type": "class", "doc": "<p>Implements a dueling bandit agent following the random policy.</p>\n"}, {"fullname": "agents.RandomAgent.RandomAgent.step", "modulename": "agents.RandomAgent", "qualname": "RandomAgent.step", "type": "function", "doc": "<p>(Override) Returns the pair that should be matched, randomly.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Pair of indices (i,j) that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.RandomAgent.RandomAgent.get_name", "modulename": "agents.RandomAgent", "qualname": "RandomAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.SparringAgent", "modulename": "agents.SparringAgent", "qualname": "", "type": "module", "doc": "<p>Sparring (MAB reduction) dueling bandit agent.\nFirst introduced in http://proceedings.mlr.press/v32/ailon14.pdf.</p>\n"}, {"fullname": "agents.SparringAgent.SparringAgent", "modulename": "agents.SparringAgent", "qualname": "SparringAgent", "type": "class", "doc": "<p>Implements a dueling bandit agent following the sparring policy.</p>\n"}, {"fullname": "agents.SparringAgent.SparringAgent.__init__", "modulename": "agents.SparringAgent", "qualname": "SparringAgent.__init__", "type": "function", "doc": "<p>Initializes Sparring agent. This agent allows a MAB to be used\nwith dueling bandits.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms.</li>\n<li><strong>mab1:</strong>  MAB in charge of arm 1. Must be of type MABAgent.</li>\n<li><strong>mab2:</strong>  MAB in charge of arm 2. Must be of type MABAgent.</li>\n</ul>\n", "parameters": ["self", "n_arms", "mab1", "mab2"], "funcdef": "def"}, {"fullname": "agents.SparringAgent.SparringAgent.reward", "modulename": "agents.SparringAgent", "qualname": "SparringAgent.reward", "type": "function", "doc": "<p>Updates the knowledge given the reward. Since it's a Dueling Bandit, the reward\nis a boolean indicating whether the first arm wins or not.\nIn MultiSBM it feeds the comparison result to the agent in charge of facing arm 1.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm_1:</strong>  first arm of the pulled pair.</li>\n<li><strong>n_arm_2:</strong>  second arm of the pulled pair.</li>\n<li><strong>one_wins:</strong>  boolean indicating whether the first arm won.</li>\n</ul>\n", "parameters": ["self", "n_arm_1", "n_arm_2", "one_wins"], "funcdef": "def"}, {"fullname": "agents.SparringAgent.SparringAgent.step", "modulename": "agents.SparringAgent", "qualname": "SparringAgent.step", "type": "function", "doc": "<p>(Override) Returns the pair that should be matched using Sparring.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Pair of indices (i,j) that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.SparringAgent.SparringAgent.reset", "modulename": "agents.SparringAgent", "qualname": "SparringAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.SparringAgent.SparringAgent.get_name", "modulename": "agents.SparringAgent", "qualname": "SparringAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.ThompsonBetaAgent", "modulename": "agents.ThompsonBetaAgent", "qualname": "", "type": "module", "doc": "<p>Thompson Sampling Agent intended for bernoulli environments.\nAs such, it uses the beta distribution as its parameter distribution.\nAs presented in https://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.</p>\n"}, {"fullname": "agents.ThompsonBetaAgent.ThompsonBetaAgent", "modulename": "agents.ThompsonBetaAgent", "qualname": "ThompsonBetaAgent", "type": "class", "doc": "<p>Implements a multi armed bandit agent following the Thompson Sampling (with beta prior) policy.</p>\n"}, {"fullname": "agents.ThompsonBetaAgent.ThompsonBetaAgent.__init__", "modulename": "agents.ThompsonBetaAgent", "qualname": "ThompsonBetaAgent.__init__", "type": "function", "doc": "<p>Initializes the agent. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms.</li>\n<li><strong>alpha_zero:</strong>  starting alpha parameter value for the alpha distribution.</li>\n<li><strong>beta_zero:</strong>  starting beta parameter value for the beta distribution.</li>\n<li><strong>failure_thres:</strong>  governs what counts as a bernoulli failure. Any reward\nbelow the threshold will counted as a failure. This exists mainly to support\ntesting this agent in non-bernoulli environments (don't set this value for bernoulli environments).</li>\n<li><strong>optimism:</strong>  starting estimation for the value of every arm.</li>\n</ul>\n", "parameters": ["self", "n_arms", "alpha_zero", "beta_zero", "failure_thres", "optimism"], "funcdef": "def"}, {"fullname": "agents.ThompsonBetaAgent.ThompsonBetaAgent.step", "modulename": "agents.ThompsonBetaAgent", "qualname": "ThompsonBetaAgent.step", "type": "function", "doc": "<p>(Override) Returns the arm that should be pulled, using Thompson Sampling with beta prior.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Index i of the arm that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.ThompsonBetaAgent.ThompsonBetaAgent.reward", "modulename": "agents.ThompsonBetaAgent", "qualname": "ThompsonBetaAgent.reward", "type": "function", "doc": "<p>Updates the knowledge given the reward. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm:</strong>  pulled arm.</li>\n<li><strong>reward:</strong>  numerical reward obtained.</li>\n</ul>\n", "parameters": ["self", "n_arm", "reward"], "funcdef": "def"}, {"fullname": "agents.ThompsonBetaAgent.ThompsonBetaAgent.reset", "modulename": "agents.ThompsonBetaAgent", "qualname": "ThompsonBetaAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.ThompsonBetaAgent.ThompsonBetaAgent.get_name", "modulename": "agents.ThompsonBetaAgent", "qualname": "ThompsonBetaAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.ThompsonGaussianAgent", "modulename": "agents.ThompsonGaussianAgent", "qualname": "", "type": "module", "doc": "<p>Thompson Sampling Agent intended for gaussian environments.\nAs such, it uses the gaussian distribution as its parameter distribution, and\nalso a inverted gamma for the variance distribution.\nAs presented in https://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.</p>\n"}, {"fullname": "agents.ThompsonGaussianAgent.ThompsonGaussianAgent", "modulename": "agents.ThompsonGaussianAgent", "qualname": "ThompsonGaussianAgent", "type": "class", "doc": "<p>Implements a multi armed bandit agent following the Thompson Sampling (with Gaussian prior) policy.</p>\n"}, {"fullname": "agents.ThompsonGaussianAgent.ThompsonGaussianAgent.__init__", "modulename": "agents.ThompsonGaussianAgent", "qualname": "ThompsonGaussianAgent.__init__", "type": "function", "doc": "<p>Initializes thompson sampling agent with gaussian prior.\nMore on the statistical background (bayesian conjugate for normal distribution with\nunknown mean and variance) can be found on section 3 of the book:\nhttp://www.stat.columbia.edu/~gelman/book/BDA3.pdf\nWhich has been conveniently summarized for the 1-D case in here:\nhttps://richrelevance.com/2013/07/31/bayesian-analysis-of-normal-distributions-with-python/</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms.</li>\n<li><strong>avg_zero:</strong>  is the starting prediction for the reward average.</li>\n<li><strong>k_zero:</strong>  is the starting certainty for avg_zero.</li>\n<li><strong>sigma_zero:</strong>  is the starting prediction for the degrees of freedom of the variance.</li>\n<li><strong>nu_zero:</strong>  is the scale for the degree of the variance parameter.</li>\n<li><strong>optimism:</strong>  starting estimation for the value of each arm.</li>\n</ul>\n", "parameters": ["self", "n_arms", "avg_zero", "k_zero", "sigma_zero", "nu_zero", "optimism"], "funcdef": "def"}, {"fullname": "agents.ThompsonGaussianAgent.ThompsonGaussianAgent.step", "modulename": "agents.ThompsonGaussianAgent", "qualname": "ThompsonGaussianAgent.step", "type": "function", "doc": "<p>(Override) Returns the arm that should be pulled, using Thompson Sampling with gaussian prior.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Index i of the arm that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.ThompsonGaussianAgent.ThompsonGaussianAgent.reward", "modulename": "agents.ThompsonGaussianAgent", "qualname": "ThompsonGaussianAgent.reward", "type": "function", "doc": "<p>Updates the knowledge given the reward. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm:</strong>  pulled arm.</li>\n<li><strong>reward:</strong>  numerical reward obtained.</li>\n</ul>\n", "parameters": ["self", "n_arm", "reward"], "funcdef": "def"}, {"fullname": "agents.ThompsonGaussianAgent.ThompsonGaussianAgent.reset", "modulename": "agents.ThompsonGaussianAgent", "qualname": "ThompsonGaussianAgent.reset", "type": "function", "doc": "<p>Fully resets the agent</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.ThompsonGaussianAgent.ThompsonGaussianAgent.get_name", "modulename": "agents.ThompsonGaussianAgent", "qualname": "ThompsonGaussianAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.UCBAgent", "modulename": "agents.UCBAgent", "qualname": "", "type": "module", "doc": "<p>UCB multi armed bandit agent.</p>\n"}, {"fullname": "agents.UCBAgent.UCBAgent", "modulename": "agents.UCBAgent", "qualname": "UCBAgent", "type": "class", "doc": "<p>Implements a multi armed bandit agent following the UCB policy.</p>\n"}, {"fullname": "agents.UCBAgent.UCBAgent.__init__", "modulename": "agents.UCBAgent", "qualname": "UCBAgent.__init__", "type": "function", "doc": "<p>Initializes the agent.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms.</li>\n<li><strong>exploration_rate:</strong>  weight that is given to uncertainty vs average \nThe higher the rate, the more exploration occurs.</li>\n<li><strong>optimism:</strong>  starting estimation for the value of each arm.</li>\n</ul>\n", "parameters": ["self", "n_arms", "exploration_rate", "optimism"], "funcdef": "def"}, {"fullname": "agents.UCBAgent.UCBAgent.step", "modulename": "agents.UCBAgent", "qualname": "UCBAgent.step", "type": "function", "doc": "<p>(Override) Returns the arm that should be pulled, using UCB.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Index i of the arm that the policy decided to pull.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "agents.UCBAgent.UCBAgent.get_name", "modulename": "agents.UCBAgent", "qualname": "UCBAgent.get_name", "type": "function", "doc": "<p>String representation of the agent.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the agent.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments", "modulename": "environments", "qualname": "", "type": "module", "doc": "<p>Environments module</p>\n"}, {"fullname": "environments.BernoulliEnvironment", "modulename": "environments.BernoulliEnvironment", "qualname": "", "type": "module", "doc": "<p>Bernoulli-distributed Arms Environment.</p>\n"}, {"fullname": "environments.BernoulliEnvironment.BernoulliEnvironment", "modulename": "environments.BernoulliEnvironment", "qualname": "BernoulliEnvironment", "type": "class", "doc": "<p>Implements bernoulli-distributed arm environment.</p>\n"}, {"fullname": "environments.BernoulliEnvironment.BernoulliEnvironment.__init__", "modulename": "environments.BernoulliEnvironment", "qualname": "BernoulliEnvironment.__init__", "type": "function", "doc": "<p>Initializes the environment.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  number of arms.</li>\n</ul>\n", "parameters": ["self", "n_arms"], "funcdef": "def"}, {"fullname": "environments.BernoulliEnvironment.BernoulliEnvironment.pull", "modulename": "environments.BernoulliEnvironment", "qualname": "BernoulliEnvironment.pull", "type": "function", "doc": "<p>Pulls a given arm with a bernoulli distribution with mean given by arm value.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm:</strong>  arm index to be pulled.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>numerical reward obtained.</p>\n</blockquote>\n", "parameters": ["self", "n_arm"], "funcdef": "def"}, {"fullname": "environments.BernoulliEnvironment.BernoulliEnvironment.get_probability_dueling", "modulename": "environments.BernoulliEnvironment", "qualname": "BernoulliEnvironment.get_probability_dueling", "type": "function", "doc": "<p>Receives two arms and returns the probability that arm1 &gt;= arm2.\nThis should be overriden depending on the \"pull\" function, to match\nthe distribution. </p>\n\n<p>For the bernoulli distribution, we use the probability that arm1 &gt; arm2,\nplus the probability that arm1==arm2 HALVED, since we assume ties are broken\nrandomly. This also guarantees that if E[arm1] = E[arm2], 1/2 is returned.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>arm1:</strong>  first arm to be compared</li>\n<li><strong>arm2:</strong>  second arm to be compared</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>\"Probability\" that arm1 &gt;= arm2 (slightly modified so that P(arm1>=arm2) + P(arm2>=arm1) = 1)</p>\n</blockquote>\n", "parameters": ["self", "arm1", "arm2"], "funcdef": "def"}, {"fullname": "environments.BernoulliEnvironment.BernoulliEnvironment.get_name", "modulename": "environments.BernoulliEnvironment", "qualname": "BernoulliEnvironment.get_name", "type": "function", "doc": "<p>String representation of the environment.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the environment.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.CyclicRPSEnvironment", "modulename": "environments.CyclicRPSEnvironment", "qualname": "", "type": "module", "doc": "<p>Environment with a cyclic, rock-paper-scissors-like distribution.</p>\n\n<p>Only meaningful upon pairwise comparisons (behaves the same as GaussianEnvironment for single pulls)</p>\n"}, {"fullname": "environments.CyclicRPSEnvironment.CyclicRPSEnvironment", "modulename": "environments.CyclicRPSEnvironment", "qualname": "CyclicRPSEnvironment", "type": "class", "doc": "<p>Implements cyclic environment without condorcet winner.\nIMPORTANT: this environment behaves exactly like a gaussian environment\nif only the method \"step\" is used to get rewards. However, the method\n\"dueling_step\" is the one that will return the appropriate outcome with\ncyclic distribution.</p>\n"}, {"fullname": "environments.CyclicRPSEnvironment.CyclicRPSEnvironment.__init__", "modulename": "environments.CyclicRPSEnvironment", "qualname": "CyclicRPSEnvironment.__init__", "type": "function", "doc": "<p>Initializes the environment.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  Number of arms</li>\n<li><strong>winner_prob:</strong>  probability that i beats j in the case that i is better than j.</li>\n<li><strong>value_generator:</strong>  Generator function for each arm hidden value (true reward)</li>\n<li><strong>values:</strong>  Actual arm values. If given, value_generator is unused.</li>\n<li><strong>variance:</strong>  Random noise from gaussian(0,std) is applied to every probability, then clipped\nto either [1/2,1] or [0,1/2] depending on whether it's the winner or the loser.</li>\n</ul>\n", "parameters": ["self", "n_arms", "value_generator", "values", "winner_prob", "std"], "funcdef": "def"}, {"fullname": "environments.CyclicRPSEnvironment.CyclicRPSEnvironment.dueling_step", "modulename": "environments.CyclicRPSEnvironment", "qualname": "CyclicRPSEnvironment.dueling_step", "type": "function", "doc": "<p>Returns rewards for a pair of arms, updating internal values.\nThis is used for Dueling Bandits steps. This environment returns\nvalues 1 and 0 as rewards, to compare dueling algorithms only, with\ncyclic probability distribution.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm1:</strong>  first arm of the pair.</li>\n<li><strong>n_arm2:</strong>  second arm of the pair.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>rewards for each bandit of the pair, being 1 for the winner and 0\n  for the loser.</p>\n</blockquote>\n", "parameters": ["self", "n_arm1", "n_arm2"], "funcdef": "def"}, {"fullname": "environments.CyclicRPSEnvironment.CyclicRPSEnvironment.reset", "modulename": "environments.CyclicRPSEnvironment", "qualname": "CyclicRPSEnvironment.reset", "type": "function", "doc": "<p>Resets environment internals</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.CyclicRPSEnvironment.CyclicRPSEnvironment.get_probability_dueling", "modulename": "environments.CyclicRPSEnvironment", "qualname": "CyclicRPSEnvironment.get_probability_dueling", "type": "function", "doc": "<p>Receives two arms and returns the probability that arm1 &gt;= arm2.\nThe \"generalized rock paper scissors\" method is applied.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>arm1:</strong>  first arm to be compared</li>\n<li><strong>arm2:</strong>  second arm to be compared</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>\"Probability\" that arm1 &gt;= arm2 (slightly modified so that P(arm1>=arm2) + P(arm2>=arm1) = 1)</p>\n</blockquote>\n", "parameters": ["self", "arm1", "arm2"], "funcdef": "def"}, {"fullname": "environments.CyclicRPSEnvironment.CyclicRPSEnvironment.get_name", "modulename": "environments.CyclicRPSEnvironment", "qualname": "CyclicRPSEnvironment.get_name", "type": "function", "doc": "<p>String representation of the environment.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the environment.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.Environment", "modulename": "environments.Environment", "qualname": "", "type": "module", "doc": "<p>Generic Environment class.\nCan be overriden to support custom implementations.</p>\n"}, {"fullname": "environments.Environment.Environment", "modulename": "environments.Environment", "qualname": "Environment", "type": "class", "doc": "<p>Implements abstract Environment w/ constant output</p>\n"}, {"fullname": "environments.Environment.Environment.__init__", "modulename": "environments.Environment", "qualname": "Environment.__init__", "type": "function", "doc": "<p>Initializes the environment.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  Number of arms</li>\n<li><strong>value_generator:</strong>  Function for each arm hidden value (true reward)</li>\n<li><strong>values:</strong>  Arm values. If given, value_generator is ignored.</li>\n</ul>\n", "parameters": ["self", "n_arms", "value_generator", "values"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.pull", "modulename": "environments.Environment", "qualname": "Environment.pull", "type": "function", "doc": "<p>Pulls a given arm and returns reward. Override in subclasses.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm:</strong>  arm to be pulled.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>value of the reward obtained.</p>\n</blockquote>\n", "parameters": ["self", "n_arm"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.step", "modulename": "environments.Environment", "qualname": "Environment.step", "type": "function", "doc": "<p>Returns reward for given arm, updating internal values\nThis is used for Multi Armed Bandits steps.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm:</strong>  arm to be pulled.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>value of the reward obtained.</p>\n</blockquote>\n", "parameters": ["self", "n_arm"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.dueling_step", "modulename": "environments.Environment", "qualname": "Environment.dueling_step", "type": "function", "doc": "<p>Returns rewards for a pair of arms, updating internal values.\nThis is used for Dueling Bandits steps. Note that the actual\nrewards are returned so that numerical metrics can be computed\nin order to compare with multi-armed bandits. HOWEVER, dueling\nbandits should never see these values, only the result of the\npairwise comparison.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm1:</strong>  first arm of the pair.</li>\n<li><strong>n_arm2:</strong>  second arm of the pair.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Tuple containing the reward of each arm.</p>\n</blockquote>\n", "parameters": ["self", "n_arm1", "n_arm2"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.soft_reset", "modulename": "environments.Environment", "qualname": "Environment.soft_reset", "type": "function", "doc": "<p>Only resets metrics but environment is kept the same.</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.reset", "modulename": "environments.Environment", "qualname": "Environment.reset", "type": "function", "doc": "<p>Resets environment internals.</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.get_optimal", "modulename": "environments.Environment", "qualname": "Environment.get_optimal", "type": "function", "doc": "<p>Returns the index of the best arm in the environment.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>index of the best arm in the environment.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.get_optimal_value", "modulename": "environments.Environment", "qualname": "Environment.get_optimal_value", "type": "function", "doc": "<p>Returns the best value of the environment.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>the best value of the environment.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.get_copeland_winners", "modulename": "environments.Environment", "qualname": "Environment.get_copeland_winners", "type": "function", "doc": "<p>Returns a numpy array with the arms with highest copeland score.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>a numpy array with the arms with highest copeland score.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.get_copeland_regret", "modulename": "environments.Environment", "qualname": "Environment.get_copeland_regret", "type": "function", "doc": "<p>Returns the copeland individual arm regret.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>arm:</strong>  index of the arm whose regret is to be obtained.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>the copeland regret for the arm.</p>\n</blockquote>\n", "parameters": ["self", "arm"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.get_probability_dueling", "modulename": "environments.Environment", "qualname": "Environment.get_probability_dueling", "type": "function", "doc": "<p>Receives two arms and returns the probability that arm1 &gt;= arm2.\nThis should be overriden depending on the \"pull\" function, to match\nthe distribution. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>arm1:</strong>  first arm to be compared</li>\n<li><strong>arm2:</strong>  second arm to be compared</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Probability that arm1 &gt;= arm2.</p>\n</blockquote>\n", "parameters": ["self", "arm1", "arm2"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.get_probability_dueling_cached", "modulename": "environments.Environment", "qualname": "Environment.get_probability_dueling_cached", "type": "function", "doc": "<p>Receives two arms and returns the probability that arm1 &gt;= arm2.\nUses cached data to save time. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>arm1:</strong>  first arm to be compared</li>\n<li><strong>arm2:</strong>  second arm to be compared</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Probability that arm1 &gt;= arm2.</p>\n</blockquote>\n", "parameters": ["self", "arm1", "arm2"], "funcdef": "def"}, {"fullname": "environments.Environment.Environment.get_name", "modulename": "environments.Environment", "qualname": "Environment.get_name", "type": "function", "doc": "<p>String representation of the environment.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the environment.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.GaussianEnvironment", "modulename": "environments.GaussianEnvironment", "qualname": "", "type": "module", "doc": "<p>Gaussian Arms Environment.</p>\n"}, {"fullname": "environments.GaussianEnvironment.GaussianEnvironment", "modulename": "environments.GaussianEnvironment", "qualname": "GaussianEnvironment", "type": "class", "doc": "<p>Implements gaussian-distributed arm environment.</p>\n"}, {"fullname": "environments.GaussianEnvironment.GaussianEnvironment.pull", "modulename": "environments.GaussianEnvironment", "qualname": "GaussianEnvironment.pull", "type": "function", "doc": "<p>Pulls a given arm with a gaussian distribution with mean given by arm value.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm:</strong>  arm index to be pulled.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>numerical reward obtained.</p>\n</blockquote>\n", "parameters": ["self", "n_arm"], "funcdef": "def"}, {"fullname": "environments.GaussianEnvironment.GaussianEnvironment.get_probability_dueling", "modulename": "environments.GaussianEnvironment", "qualname": "GaussianEnvironment.get_probability_dueling", "type": "function", "doc": "<p>Receives two arms and returns the probability that arm1 &gt;= arm2.\nThis should be overriden depending on the \"pull\" function, to match\nthe distribution. </p>\n\n<p>For the normal distribution, is 1 - cdf((m2-m1)/sqrt(var1+var2))</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>arm1:</strong>  first arm to be compared</li>\n<li><strong>arm2:</strong>  second arm to be compared</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Probability that arm1 &gt;= arm2.</p>\n</blockquote>\n", "parameters": ["self", "arm1", "arm2"], "funcdef": "def"}, {"fullname": "environments.GaussianEnvironment.GaussianEnvironment.get_name", "modulename": "environments.GaussianEnvironment", "qualname": "GaussianEnvironment.get_name", "type": "function", "doc": "<p>String representation of the environment.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the environment.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.NoisyGaussianEnvironment", "modulename": "environments.NoisyGaussianEnvironment", "qualname": "", "type": "module", "doc": "<p>Noisy Gaussian Arms Environment.\nIn this environment, when pulling dueling bandits, each pair has a \nrandom noise added so as to break transitivity.</p>\n"}, {"fullname": "environments.NoisyGaussianEnvironment.NoisyGaussianEnvironment", "modulename": "environments.NoisyGaussianEnvironment", "qualname": "NoisyGaussianEnvironment", "type": "class", "doc": "<p>Implements noisy gaussian environment.\nIMPORTANT: this environment behaves exactly like a gaussian environment\nif only the method \"step\" is used to get rewards. However, the method\n\"dueling_step\" is the one that will return the appropriate outcome with\nnoisy distribution.</p>\n"}, {"fullname": "environments.NoisyGaussianEnvironment.NoisyGaussianEnvironment.__init__", "modulename": "environments.NoisyGaussianEnvironment", "qualname": "NoisyGaussianEnvironment.__init__", "type": "function", "doc": "<p>Initializes the environment.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arms:</strong>  Number of arms</li>\n<li><strong>value_generator:</strong>  Function for each arm hidden value (true reward)</li>\n<li><strong>values:</strong>  Arm values. If given, value_generator is ignored.</li>\n<li><strong>d:</strong>  Amount of noise added. The higher, the less transitivity.</li>\n</ul>\n", "parameters": ["self", "n_arms", "value_generator", "values", "d"], "funcdef": "def"}, {"fullname": "environments.NoisyGaussianEnvironment.NoisyGaussianEnvironment.dueling_step", "modulename": "environments.NoisyGaussianEnvironment", "qualname": "NoisyGaussianEnvironment.dueling_step", "type": "function", "doc": "<p>Returns rewards for a pair of arms, updating internal values.\nThis is used for Dueling Bandits steps. Note that the actual\nrewards are returned so that numerical metrics can be computed\nin order to compare with multi-armed bandits. HOWEVER, dueling\nbandits should never see these values, only the result of the\npairwise comparison.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_arm1:</strong>  first arm of the pair.</li>\n<li><strong>n_arm2:</strong>  second arm of the pair.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Tuple containing the reward of each arm.</p>\n</blockquote>\n", "parameters": ["self", "n_arm1", "n_arm2"], "funcdef": "def"}, {"fullname": "environments.NoisyGaussianEnvironment.NoisyGaussianEnvironment.get_probability_dueling", "modulename": "environments.NoisyGaussianEnvironment", "qualname": "NoisyGaussianEnvironment.get_probability_dueling", "type": "function", "doc": "<p>Receives two arms and returns the probability that arm1 &gt;= arm2.\nThis should be overriden depending on the \"pull\" function, to match\nthe distribution. </p>\n\n<p>For the normal distribution, is 1 - cdf((m2-m1)/sqrt(var1+var2))</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>arm1:</strong>  first arm to be compared</li>\n<li><strong>arm2:</strong>  second arm to be compared</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>Probability that arm1 &gt;= arm2.</p>\n</blockquote>\n", "parameters": ["self", "arm1", "arm2"], "funcdef": "def"}, {"fullname": "environments.NoisyGaussianEnvironment.NoisyGaussianEnvironment.reset", "modulename": "environments.NoisyGaussianEnvironment", "qualname": "NoisyGaussianEnvironment.reset", "type": "function", "doc": "<p>Resets environment internals</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "environments.NoisyGaussianEnvironment.NoisyGaussianEnvironment.get_name", "modulename": "environments.NoisyGaussianEnvironment", "qualname": "NoisyGaussianEnvironment.get_name", "type": "function", "doc": "<p>String representation of the environment.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>string representing the environment.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation", "modulename": "simulation", "qualname": "", "type": "module", "doc": "<p>Simulation module</p>\n"}, {"fullname": "simulation.Experiment", "modulename": "simulation.Experiment", "qualname": "", "type": "module", "doc": "<p>Class representing a MAB/DB experiment.</p>\n"}, {"fullname": "simulation.Experiment.Experiment", "modulename": "simulation.Experiment", "qualname": "Experiment", "type": "class", "doc": "<p>Class that encapsulates all the data needed for a single experiment.\nHere, a \"single experiment\" means a fixed set of agents against a\nfixed environment, for a fixed amount of iterations wich may be\nrepeated more than one time with distinct seeds for averaging.</p>\n"}, {"fullname": "simulation.Experiment.Experiment.__init__", "modulename": "simulation.Experiment", "qualname": "Experiment.__init__", "type": "function", "doc": "<p>Initializes the experiment.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>Name:</strong>  identifier for the experiment. Must be unique.</li>\n<li><strong>Agents:</strong>  list of agents to simulate</li>\n<li><strong>Environment:</strong>  Environment object with the arms</li>\n<li><strong>n_epochs:</strong>  N\u00ba of iterations per agent on a given environment</li>\n<li><strong>n_repeats:</strong>  N\u00ba of environments per agent for robustness</li>\n<li><strong>plot_position:</strong>  If this experiment can be parameterized within the simulation by a cardinal value \n(for example, the number of arms), it should be indicated here for consistent plots.</li>\n</ul>\n", "parameters": ["self", "name", "agents", "environment", "n_epochs", "n_repeats", "plot_position"], "funcdef": "def"}, {"fullname": "simulation.Experiment.Experiment.run", "modulename": "simulation.Experiment", "qualname": "Experiment.run", "type": "function", "doc": "<p>Runs the experiment and stores the metrics.</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Experiment.Experiment.plot_metrics", "modulename": "simulation.Experiment", "qualname": "Experiment.plot_metrics", "type": "function", "doc": "<p>Plots and shows given metric for the experiment.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>metric_name:</strong>  Name of the desired metric within the available ones (check module \"Metrics\" or readme).</li>\n<li><strong>scale:</strong>  pyplot scale format for both axes.</li>\n</ul>\n", "parameters": ["self", "metric_name", "scale"], "funcdef": "def"}, {"fullname": "simulation.Experiment.Experiment.save_metrics", "modulename": "simulation.Experiment", "qualname": "Experiment.save_metrics", "type": "function", "doc": "<p>Plots and stores to png given metric for the experiment.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>metric_name:</strong>  Name of the desired metric within the available ones (check module \"Metrics\" or readme).</li>\n<li><strong>scale:</strong>  pyplot scale format for both axes.</li>\n</ul>\n", "parameters": ["self", "metric_name", "scale"], "funcdef": "def"}, {"fullname": "simulation.Experiment.Experiment.get_name", "modulename": "simulation.Experiment", "qualname": "Experiment.get_name", "type": "function", "doc": "<p>Returns the name of the experiment.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>the name of the experiment.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Experiment.Experiment.was_run", "modulename": "simulation.Experiment", "qualname": "Experiment.was_run", "type": "function", "doc": "<p></p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Experiment.Experiment.get_final_values", "modulename": "simulation.Experiment", "qualname": "Experiment.get_final_values", "type": "function", "doc": "<p>Returns dictionary \"agent_index: value\" where the value is the final value for metric_name.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>metric_name:</strong>  Name of the desired metric within the available ones (check module \"Metrics\" or readme).</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>dictionary \"agent_index: value\" where the value is the final value for metric_name.</p>\n</blockquote>\n", "parameters": ["self", "metric_name"], "funcdef": "def"}, {"fullname": "simulation.Experiment.Experiment.get_agent_count", "modulename": "simulation.Experiment", "qualname": "Experiment.get_agent_count", "type": "function", "doc": "<p>Returns number of agents.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>number of agents.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Experiment.Experiment.get_agent_by_index", "modulename": "simulation.Experiment", "qualname": "Experiment.get_agent_by_index", "type": "function", "doc": "<p>Returns agent object by given index.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>index:</strong>  index of the desired agent.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>agent object.</p>\n</blockquote>\n", "parameters": ["self", "index"], "funcdef": "def"}, {"fullname": "simulation.Experiment.Experiment.get_plot_position", "modulename": "simulation.Experiment", "qualname": "Experiment.get_plot_position", "type": "function", "doc": "<p>Returns this object \"plot position\" value.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>this object \"plot position\" value.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Metrics", "modulename": "simulation.Metrics", "qualname": "", "type": "module", "doc": "<p>Contains and updates MAB metrics for each experiment.</p>\n\n<h6 id=\"supported-metric-names\">Supported metric names</h6>\n\n<blockquote>\n  <p>'reward': average reward obtained by the pulled pair (for MABs, reward pulled).\n  'regret': standard MAB cumulative regret, using average reward of the pair.\n  'dueling_regret': cumulative copeland regret. For MABs, a pull is taken as a pair match of the form (i,i).\n      Coincides with standard cumulative dueling bandit regret in Condorcet situations.\n  'copeland_regret': same as dueling_regret.\n  'dueling_regret_non_cumulative': non cumulative version of dueling_regret.\n  'copeland_regret_non_cumulative': same as copeland_regret.\n  'weak_regret': cumulative weak copeland regret, that is, only the minimum regret of the pair is stored instead \n      of the average.\n  'strong_regret': cumulative strong copeland regret, that is, only the maximum regret of the pair is stored instead \n      of the average.\n  'optimal_percent': percentage of runs that chose the optimal arm in a given epoch.</p>\n</blockquote>\n"}, {"fullname": "simulation.Metrics.new_average", "modulename": "simulation.Metrics", "qualname": "new_average", "type": "function", "doc": "<p>Helper function to update average value efficiently.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>old_average:</strong>  old value of the average</li>\n<li><strong>new_value:</strong>  new value to be added</li>\n<li><strong>value_count:</strong>  total value count including new value</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>updated average</p>\n</blockquote>\n", "parameters": ["old_average", "new_value", "value_count"], "funcdef": "def"}, {"fullname": "simulation.Metrics.Metrics", "modulename": "simulation.Metrics", "qualname": "Metrics", "type": "class", "doc": "<p>Stores metrics for an experiment.</p>\n"}, {"fullname": "simulation.Metrics.Metrics.__init__", "modulename": "simulation.Metrics", "qualname": "Metrics.__init__", "type": "function", "doc": "<p>Initializes the metrics object.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>n_epochs:</strong>  number of epochs to store.</li>\n</ul>\n", "parameters": ["self", "n_epochs"], "funcdef": "def"}, {"fullname": "simulation.Metrics.Metrics.update_dueling", "modulename": "simulation.Metrics", "qualname": "Metrics.update_dueling", "type": "function", "doc": "<p>Updates the data after a dueling bandits pull, that is, after\na comparison between two arms has been carried out.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>epoch:</strong>  epoch number.</li>\n<li><strong>environment:</strong>  Environment object where the experiment is run.</li>\n<li><strong>arm1:</strong>  first pulled arm.</li>\n<li><strong>arm2:</strong>  second pulled arm.</li>\n<li><strong>reward1:</strong>  first reward obtained.</li>\n<li><strong>reward2:</strong>  second reward obtained.</li>\n<li><strong>optimal_arm:</strong>  index of the best possible arm.</li>\n<li><strong>optimal_reward:</strong>  value of the best possible arm.</li>\n</ul>\n", "parameters": ["self", "epoch", "environment", "arm1", "arm2", "reward1", "reward2", "optimal_arm", "optimal_reward"], "funcdef": "def"}, {"fullname": "simulation.Metrics.Metrics.update", "modulename": "simulation.Metrics", "qualname": "Metrics.update", "type": "function", "doc": "<p>Updates the metrics using a single reward (that is, for the case of MABs)</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>epoch:</strong>  epoch number.</li>\n<li><strong>environment:</strong>  Environment object where the experiment is run.</li>\n<li><strong>arm:</strong>  pulled arm.</li>\n<li><strong>reward:</strong>  reward obtained.</li>\n<li><strong>optimal_arm:</strong>  index of the best possible arm.</li>\n<li><strong>optimal_reward:</strong>  value of the best possible arm.</li>\n</ul>\n", "parameters": ["self", "epoch", "environment", "arm", "reward", "optimal_arm", "optimal_reward"], "funcdef": "def"}, {"fullname": "simulation.Metrics.Metrics.new_iteration", "modulename": "simulation.Metrics", "qualname": "Metrics.new_iteration", "type": "function", "doc": "<p>Call if a new iteration (that is, different environment) has begun.</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Metrics.Metrics.get_metrics", "modulename": "simulation.Metrics", "qualname": "Metrics.get_metrics", "type": "function", "doc": "<p>Gets all metrics in form of dictionary.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>dictionary {metric_name: list} where the list has each epoch metric.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Metrics.Metrics.get_metric", "modulename": "simulation.Metrics", "qualname": "Metrics.get_metric", "type": "function", "doc": "<p>Returns all epoch values for a given metric.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>all epoch values for a given metric.</p>\n</blockquote>\n", "parameters": ["self", "name"], "funcdef": "def"}, {"fullname": "simulation.Metrics.Metrics.get_metric_result", "modulename": "simulation.Metrics", "qualname": "Metrics.get_metric_result", "type": "function", "doc": "<p>Returns the last epoch value for a given metric.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>the last epoch value for a given metric.</p>\n</blockquote>\n", "parameters": ["self", "name"], "funcdef": "def"}, {"fullname": "simulation.Simulation", "modulename": "simulation.Simulation", "qualname": "", "type": "module", "doc": "<p>Carries out multi-experiment simulations.</p>\n"}, {"fullname": "simulation.Simulation.Simulation", "modulename": "simulation.Simulation", "qualname": "Simulation", "type": "class", "doc": "<p>Class that carries out MAB and DB experiments.</p>\n"}, {"fullname": "simulation.Simulation.Simulation.__init__", "modulename": "simulation.Simulation", "qualname": "Simulation.__init__", "type": "function", "doc": "<p>Initializes class.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>Name:</strong>  name for the global simulation.</li>\n<li><strong>Experiments:</strong>  list of experiments that will be carried out.</li>\n</ul>\n", "parameters": ["self", "name", "experiments"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.add_experiment", "modulename": "simulation.Simulation", "qualname": "Simulation.add_experiment", "type": "function", "doc": "<p>Adds experiment to the simulation queue. </p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>experiment:</strong>  experiment to be added.</li>\n<li><strong>override:</strong>  If the experiment already exists (i.e, an experiment with\nthe same name exists), then it won't be added unless override is\nset to true.</li>\n</ul>\n", "parameters": ["self", "experiment", "override"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.get_experiment_count", "modulename": "simulation.Simulation", "qualname": "Simulation.get_experiment_count", "type": "function", "doc": "<p>Returns number of experiments within the simulation.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>number of experiments.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.get_experiment_names", "modulename": "simulation.Simulation", "qualname": "Simulation.get_experiment_names", "type": "function", "doc": "<p>Returns the names of the experiments within the simulation.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>names of the experiments.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.get_experiment_by_name", "modulename": "simulation.Simulation", "qualname": "Simulation.get_experiment_by_name", "type": "function", "doc": "<p>Returns experiment object given its name.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>name:</strong>  name of the experiment.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>experiment matching the given name.</p>\n</blockquote>\n", "parameters": ["self", "name"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.get_experiment_by_index", "modulename": "simulation.Simulation", "qualname": "Simulation.get_experiment_by_index", "type": "function", "doc": "<p>Returns experiment object given its index.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>index:</strong>  index of the experiment.</li>\n</ul>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>experiment object.</p>\n</blockquote>\n", "parameters": ["self", "index"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.run_all", "modulename": "simulation.Simulation", "qualname": "Simulation.run_all", "type": "function", "doc": "<p>Runs every (remaining) experiment.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>save:</strong>  if set to true, simulation state is saved to disk after each experiment.</li>\n</ul>\n", "parameters": ["self", "save"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.save_all_metrics", "modulename": "simulation.Simulation", "qualname": "Simulation.save_all_metrics", "type": "function", "doc": "<p>For each ran experiment, saves a png with the metric \"metric_name\" plotted.\nThis doesn't plot aggregated metrics, nor saves the results to file other than the image.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>metric_name:</strong>  Name of the desired metric within the available ones (check module \"Metrics\" or readme).</li>\n<li><strong>scale:</strong>  pyplot scale format for both axes.</li>\n</ul>\n", "parameters": ["self", "metric_name", "scale"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.plot_all_metrics", "modulename": "simulation.Simulation", "qualname": "Simulation.plot_all_metrics", "type": "function", "doc": "<p>For each ran experiment, plots the metric \"metric_name\".\nThis doesn't plot aggregated metrics.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>metric_name:</strong>  Name of the desired metric within the available ones (check module \"Metrics\" or readme).</li>\n<li><strong>scale:</strong>  pyplot scale format for both axes.</li>\n</ul>\n", "parameters": ["self", "metric_name", "scale"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.save_state", "modulename": "simulation.Simulation", "qualname": "Simulation.save_state", "type": "function", "doc": "<p>Stores the state of the simulation as pickle.</p>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.load_state", "modulename": "simulation.Simulation", "qualname": "Simulation.load_state", "type": "function", "doc": "<p>Reloads the state of a pickled simulation and returns it.</p>\n\n<h6 id=\"returns\">Returns</h6>\n\n<blockquote>\n  <p>loaded simulation object.</p>\n</blockquote>\n", "parameters": ["self"], "funcdef": "def"}, {"fullname": "simulation.Simulation.Simulation.plot_aggregated_metrics", "modulename": "simulation.Simulation", "qualname": "Simulation.plot_aggregated_metrics", "type": "function", "doc": "<p>Plots the final value of the given metric for each experiment.\nThe values are plotted left to right in order of insertion.\nMake sure that \"equivalent\" agents are inserted in the same order\non each experiment so that they get connected if required.</p>\n\n<h6 id=\"args\">Args</h6>\n\n<ul>\n<li><strong>metric_name:</strong>  Name of the desired metric within the available ones (check module \"Metrics\" or readme).</li>\n<li><strong>padding:</strong>  range for x axis will be [min-padding[0], max+padding[1]].</li>\n<li><strong>scale:</strong>  pyplot scale format for both axes.</li>\n<li><strong>cut_ticks:</strong>  Dont create xticks for x values smaller than cut_ticks[0] or larger than cut_ticks[1].</li>\n</ul>\n", "parameters": ["self", "metric_name", "padding", "scale", "cut_ticks"], "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.addField("qualname");
            this.addField("fullname");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();